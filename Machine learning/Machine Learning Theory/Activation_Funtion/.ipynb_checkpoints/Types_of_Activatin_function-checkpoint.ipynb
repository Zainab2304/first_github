{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "- Activation function defines the output of input or set of inputs\n",
    "- They basically decide to activate or deactivate neurons to get the desired output. It also performs a nonlinear transformation on the input to get better results on a complex neural network.\n",
    "- Activation function also helps to normalize the output of any input in the range between 1 to -1. Activation function must be efficient and it should reduce the computation time because the neural network sometimes trained on millions of data points.\n",
    "- Without activation function, weight and bias would only have a linear transformation, or neural network is just a linear regression model, a linear equation is polynomial of one degree only which is simple to solve but limited in terms of ability to solve complex problems or higher degree polynomials.\n",
    "- But opposite to that, the addition of activation function to neural network executes the non-linear transformation to input and make it capable to solve complex problems such as language translations and image classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \n",
    "Activation function basically decides in any neural network that given input or receiving information is relevant or it is irrelevant. Let's take an example to understand better what is a neuron and how activation function bounds the output value to some limit.\n",
    "   -   The neuron is basically is a weighted average of input, then this sum is passed through an activation function to get an output.\n",
    "\n",
    " \n",
    "\n",
    "Y = ∑ (weights*input + bias) \n",
    "\n",
    " \n",
    "\n",
    "Here Y can be anything for a neuron between range -infinity to +infinity. So, we have to bound our output to get the desired prediction or generalized results.\n",
    "\n",
    " \n",
    "\n",
    "Y = Activation function(∑ (weights*input + bias)) \n",
    "\n",
    " \n",
    "\n",
    "So, we pass that neuron to activation function to bound output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Activation Functions\n",
    "1. Binary Step Function\n",
    "2. Linear Function\n",
    "3. ReLU( Rectified Linear unit) Activation function\n",
    "4. Leaky ReLU Activation Function\n",
    "5. Sigmoid Activation Function\n",
    "6. Hyperbolic Tangent Activation Function(Tanh)\n",
    "7. Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Binary Step Function\n",
    "This activation function very basic and it comes to mind every time if we try to bound output. It is basically a threshold base classifier, in this, we decide some threshold value to decide output that input should be activated or deactivated.\n",
    "\n",
    "if x > 0:\n",
    "\n",
    "f(x) = 1\n",
    "\n",
    " else:\n",
    " \n",
    "0 if x < 0\n",
    "\n",
    "In this, we decide the threshold value to 0. It is very simple and useful to classify binary problems or classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BSF](BSF.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simple straight line activation function where our function is directly proportional to the weighted sum of input. Linear activation functions are better in giving a wide range of activations and a line of a positive slope may increase the firing rate as the input rate increases.\n",
    "\n",
    " \n",
    "\n",
    "In binary, either a neuron is firing or not. If you know gradient descent in deep learning then you would notice that in this function derivative is constant.\n",
    "\n",
    " \n",
    "\n",
    "Y = mZ\n",
    "\n",
    " \n",
    "\n",
    "Where derivative with respect to Z is constant m. The meaning gradient is also constant and it has nothing to do with Z.\n",
    "\n",
    "\n",
    "f(x)=4x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LF](LF.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ReLU( Rectified Linear unit) Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non Linear Function \n",
    "- Rectified linear unit or ReLU is most widely used activation function right now which ranges from 0 to infinity, All the negative values are converted into zero, and this conversion rate is so fast that neither it can map nor fit into data properly which creates a problem, but where there is a problem there is a solution.\n",
    "- We use Leaky ReLU function instead of ReLU to avoid this unfitting, in Leaky ReLU range is expanded which enhances the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"RLU.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Leaky ReLU Activation Function\n",
    "We needed the Leaky ReLU activation function to solve the ‘Dying ReLU’ problem, as discussed in ReLU, we observe that all the negative input values turn into zero very quickly and in the case of Leaky ReLU we do not make all negative inputs to zero but to a value near to zero which solves the major issue of ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"LRLU.jpg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sigmoid Activation Function\n",
    "- The sigmoid activation function is used mostly as it does its task with great efficiency, it basically is a probabilistic approach towards decision making and ranges in between 0 to 1, so when we have to make a decision or to predict an output we use this activation function because of the range is the minimum, therefore, prediction would be more accurate.\n",
    "- The equation for the sigmoid function is\n",
    "\n",
    "f(x) = 1/(1+e**(-x) )\n",
    "\n",
    " -  The sigmoid function causes a problem mainly termed as vanishing gradient problem which occurs because we convert large input in between the range of 0 to 1 and therefore their derivatives become much smaller which does not give satisfactory output. To solve this problem another activation function such as ReLU is used where we do not have a small derivative problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"SF.jpg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Hyperbolic Tangent Activation Function(Tanh)\n",
    "This activation function is slightly better than the sigmoid function, like the sigmoid function it is also used to predict or to differentiate between two classes but it maps the negative input into negative quantity only and ranges in between -1 to  1.\n",
    "\n",
    " The tanh function is defined as-\n",
    "\n",
    " tanh(x)=2sigmoid(2x)-1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TanH](TanH.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Softmax Activation Function\n",
    "- Softmax is used mainly at the last layer i.e output layer for decision making the same as sigmoid activation works, the softmax basically gives value to the input variable according to their weight and the sum of these weights is eventually one.\n",
    "- For Binary classification, both sigmoid, as well as softmax, are equally approachable but in case of multi-class classification problem we generally use softmax and cross-entropy along with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"STM.jpg\" width=\"600\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "266c4d81548097d9dd641a4c19dbdb8cb4ce739c1b70c23a83575cb49d14a5df"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
